version: '3.7'

services:
  connector:
    image: "firestarter_connector:1.0"
    build:
      context: .
      dockerfile: connector/Dockerfile
    init: true
    networks:
      - internal
    ports:
      # Provides optional access to the raw Firehose stream for consumption
      # by non-dockerized applications
      - "${STREAMING_PORT:-127.0.0.1:1601}:1601"
    environment:
      # Firehose account username
      - FH_USERNAME=${FH_USERNAME:?FH_USERNAME variable must be set}
      # Firehose account key
      - FH_APIKEY=${FH_APIKEY:?FH_APIKEY variable must be set}
      # Firehose URL, firehose.flightaware.com can also be used
      - SERVER=${SERVER:-firehose-test.flightaware.com}
      # Streaming compression of incoming Firehose data. Valid values are gzip,
      # deflate, or compress. Leave blank to disable compression.
      - COMPRESSION=${COMPRESSION:-}
      # Frequency in seconds to print stats about connection (messages/bytes
      # per second). Set to 0 to disable.
      - PRINT_STATS_PERIOD=${PRINT_STATS_PERIOD:-10}
      # Frequency in seconds that Firehose should send a synthetic "keepalive"
      # message to help connector ensure the connection is still alive. If no
      # such message is received within roughly $keepalive seconds, connector
      # will automatically reconnect to Firehose.
      - KEEPALIVE=${KEEPALIVE:-60}
      # "Time mode" of Firehose init command. Can be "live" or "pitr <pitr>";
      # range is currently not supported.
      # See https://flightaware.com/commercial/firehose/documentation/commands
      # for more details.
      - INIT_CMD_TIME=${INIT_CMD_TIME:-live}
      # The "optional" section of the Firehose init command. Mostly consists of
      # filters for the data. Do not put username, password, keepalive, or
      # compression commands here. Documentation at
      # https://flightaware.com/commercial/firehose/documentation/commands
      - INIT_CMD_ARGS=${INIT_CMD_ARGS:-}
      # Whether connector should immediately connect to Firehose upon starting
      # (1) or wait until its first client has connected before it connects to
      # Firehose (0).
      - PYTHONUNBUFFERED=1
      # Each producer should have its own unique FLIFO kafka topic name
      - KAFKA_FLIFO_TOPIC_NAME=feed1
      # Each producer should have its own unique POSITION kafka topic name
      - KAFKA_POSITION_TOPIC_NAME=position_feed1
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"

  db-updater:
    image: "firestarter_db-updater:1.0"
    build:
      context: .
      dockerfile: db-updater/Dockerfile
    init: true
    networks:
      - internal
    environment:
      # URL to database that will be updated based on Firehose contents.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${FLIGHTS_DB_URL:-sqlite:///db/flights.db}
      - PYTHONUNBUFFERED=1
      # Same kafka topic name as the producer of the feed that you want to consume
      - KAFKA_TOPIC_NAME=feed1
      # Consumers with the same group name will split the data between them,
      # but consumers with different group names will each receive all of the messages 
      - KAFKA_GROUP_NAME=group1
      # Set this to "flights" or "positions" depending on what kinds of messages this updater is handling
      - TABLE=flights
    volumes:
      - data:/home/firestarter/app/db
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"

  position-db-updater:
    image: "firestarter_db-updater:1.0"
    build:
      context: .
      dockerfile: db-updater/Dockerfile
    init: true
    networks:
      - internal
    environment:
      # URL to database that will be updated based on Firehose contents.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${POSITION_DB_URL:-postgresql://postgres:positions@timescaledb:5432}
      - PYTHONUNBUFFERED=1
      # Same kafka topic name as the producer of the feed that you want to consume
      - KAFKA_TOPIC_NAME=position_feed1
      # Consumers with the same group name will split the data between them,
      # but consumers with different group names will each receive all of the messages 
      - KAFKA_GROUP_NAME=position_group1
      # Set this to "flights" or "positions" depending on what kinds of messages this updater is handling
      - TABLE=positions
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"
    depends_on:
      - timescaledb

  fids:
    image: "firestarter_fids:1.0"
    build:
      context: .
      dockerfile: fids/Dockerfile
    init: true
    ports:
      # Port upon which to serve webapp
      - "${WEB_SERVER_PORT:-5000}:5000"
    networks:
      - internal
    environment:
      # URL to database that is being updated by db-updater.
      # Documentation at https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls
      - DB_URL=${FLIGHTS_DB_URL:-sqlite:///db/flights.db}
      - PYTHONUNBUFFERED=1
    volumes:
      - data:/home/firestarter/app/db
    logging:
      driver: "json-file"
      options:
        max-size: "10mb"
        max-file: "5"

  zookeeper:
    image: "bitnami/zookeeper:3.6.0"
    init: true
    ports:
      - "2181:2181"
    networks:
      - internal
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper

  kafka:
    image: "bitnami/kafka:2.5.0"
    init: true
    ports:
      - "9092:9092"
    networks:
      - internal
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      # Retain messages for 1 hour
      - KAFKA_CFG_LOG_RETENTION_HOURS=1
    volumes:
      - kafka_data:/bitnami/kafka
    depends_on:
      - zookeeper

  timescaledb:
    image: "timescale/timescaledb:1.7.1-pg12"
    init: true
    ports:
      - "5432:5432"
    networks:
      - internal
    environment:
      - POSTGRES_PASSWORD=positions
    volumes:
      - position_data:/var/lib/postgresql/data

volumes:
  data:
  position_data:
  kafka_data:
  zookeeper_data:

networks:
  internal:
